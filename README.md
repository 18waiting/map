# Math Misconception Classifier using LLM + LoRA

This repository contains a fine-tuned LLM model using LoRA adapters based on Googleâ€™s [`gemma-2b`](https://huggingface.co/google/gemma-2b) to classify student misconceptions in math problems. The model takes a math question, studentâ€™s answer, and explanation, then predicts whether thereâ€™s a misconception and classifies it into predefined categories.

## ğŸ” Problem Statement

Understanding how students think is essential for educators. This project aims to automatically analyze math problem responses and:
- Detect if the answer is correct or incorrect.
- Identify the reasoning behind the studentâ€™s explanation.
- Classify specific **misconceptions** if present.

---

## ğŸ“š Dataset Information

This project is based on the dataset from the [**MAP: Charting Student Math Misunderstandings**](https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings) Kaggle competition. The dataset contains:

* **Student-written responses** to open-ended math questions.
* **Multiple annotations** per response indicating:

  * Whether the response is correct/incorrect.
  * If incorrect, what kind of **misconception** was present (e.g., `SwapDividend`, `Inverse_operation`, `Wrong_Fraction`, etc.).
  * Categorization such as `True_Correct`, `False_Misconception`, `True_Neither`, etc.

## Dataset Files:

* `train.csv`: Annotated training data with explanations and labels
* `test.csv`: Evaluation set with responses to classify
* `sample_submission.csv`: Submission format for competition entries

> For more details, visit the [Kaggle competition page](https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings/overview).

---

## ğŸ§  Model Overview

- **Base model**: `google/gemma-2b`
- **Fine-tuning strategy**: Parameter-efficient fine-tuning (PEFT) using **LoRA**
- **Quantization**: 4-bit using `bitsandbytes` for memory efficiency
- **Trainer**: `trl.SFTTrainer` from Hugging Face

### ğŸ·ï¸ Output Format
```

Category\[:Misconception]

```

Example:
```

False_Misconception\:SwapDividend

```

## ğŸ“ Project Structure

```

math_compe/
â”‚
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ checkpoint-900/         # LoRA fine-tuned weights
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ train.csv               # Raw training data
â”‚   â”œâ”€â”€ test.csv                # Evaluation data
â”‚   â”œâ”€â”€ sample_submission.csv   # Submission format
â”‚   â””â”€â”€ train_finetune.jsonl    # Instruction-formatted dataset
â”‚
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ exploration.ipynb       # Data cleaning, analysis, preprocessing
â”‚   â””â”€â”€ load_model.ipynb        # Inference with fine-tuned model

```

## ğŸ—ï¸ Pipeline Summary

### 1. Data Preparation (`exploration.ipynb`)
- Clean LaTeX equations and student explanations
- Normalize math expressions (e.g. `\frac{3}{4}` â†’ `3/4`)
- Construct input-output pairs in instruction format

### 2. Fine-tuning (`SFTTrainer`)
- Tokenized instruction+input+output using `AutoTokenizer`
- Configured LoRA on key transformer layers
- Logged training with Weights & Biases (`wandb`)

### 3. Evaluation
- Visualized training and validation loss
- Tracked token-level accuracy on `wandb`
- Performed manual inference tests post-finetuning

## ğŸ§ª Example Inference

```

Input:
Question: Calculate 1/2 Ã· 6
Answer: 1/3
Explanation: dividing 1/2 by 6 gives 6/3 because 2 goes into 6 three times...

Output:
False_Misconception\:SwapDividend

```

## ğŸš€ How to Use

1. Clone this repository
2. Install dependencies (see `requirements.txt`)
3. Load and run the inference file `load_model.ipynb` using the fine-tuned weights in `artifacts/checkpoint-900/`

## ğŸ”§ Dependencies

* `transformers`
* `peft`
* `trl`
* `bitsandbytes`
* `datasets`
* `wandb`

## ğŸ“Š Tracking

Training progress and metrics are available at:

**WandB Run**: [`finetune-gemma-math-add-patter-answer`](https://wandb.ai/your-profile/finetune-gemma-math-add-patter-answer)

## âœï¸ Author

**Muhammad Fikry Rizal** â€“ AI Curriculum Developer, passionate about LLMs and education technology.

---

ğŸ“¬ Feel free to contribute or raise an issue!